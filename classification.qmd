---
title: "Classification"
---

## logist regression

```{r}
#数据准备####
breast<-read.table("wdbc.data",head=FALSE,sep=",") 
breast<-breast[,1:12]
head(breast,3)
names(breast)<-c("ID","Diagnosis","radius","texture",
                 "perimeter","area","smoothness","compactness",
                 "concavity","concave_points","symmetry","fractal_dimension")
str(breast)
df<-breast[,-1]       # 569
df$Diagnosis<-factor(df$Diagnosis,levels = c("B","M"),labels = c(2,4)) #良性B=2，恶性M=4

set.seed(1234)
train<-sample(nrow(df),0.7*nrow(df)) #  70%的行 sample(length(frame),size,replace=T/F 有无放回)
df.train<-df[train,] #训练集 398
table(df.train$Diagnosis)
df.validate<-df[-train,] #验证集 171
table(df.validate$Diagnosis)
```

```{r}
fit.logit<-glm(Diagnosis~.,data = df.train,family = binomial())
summary(fit.logit)

prob<-predict(fit.logit,df.validate,type = "response")
prob
logit.pred<-factor(prob>.5,levels = c(FALSE,TRUE),labels = c("B","M"))
logit.pred
t1<-table(df.validate$Diagnosis,logit.pred,dnn = c("actual","predicted")) #混淆矩阵
```

# 决策树

```{r}
library(rpart)
set.seed(1234)
dtree<-rpart(Diagnosis~.,data = df.train,method = "class",
             parms = list(split="information"))  #分类任务 信息增益（information gain）生成树
dtree$cptable
plotcp(dtree)  #对于所有交叉验证误差，在最小交叉验证误差一个标准差内的最小的树即最优的树
 #cp=0.0227，nsplit=1 即最优的树
dtree.pruned<-prune(dtree,cp=.0227)  #剪枝    一次分割的复杂度参数cp=0.0227

library(rpart.plot)
prp(dtree.pruned,type=2,extra = 104,fallen.leaves =TRUE,main="Decision Tree") 

dtree.pred<-predict(dtree.pruned,df.validate,type = "class")
t2<-table(df.validate$Diagnosis,dtree.pred,dnn = c("actual","predicted")) 
```

###   Logistic 回归（因变量为类别型）

```{r}
library(AER)  #install.packages("AER")
data("Affairs",package = "AER")
summary(Affairs)
table(Affairs$affairs)

Affairs$yn[Affairs$affairs>0]<-1
Affairs$yn[Affairs$affairs==0]<-0
Affairs$yn<-factor(Affairs$yn,levels = c(0,1),labels = c("No","Yes"))

table(Affairs$yn)

fit1<-glm(yn~gender+age+yearsmarried+children+
          religiousness+education+occupation+rating,
          data=Affairs,family = binomial(link = logit)
          )
summary(fit1)

fit2<-glm(yn~age+yearsmarried+religiousness+rating,
         data=Affairs,family = binomial(link = logit)
)
summary(fit2)

anova(fit2,fit1,test = "Chisq") #两模型嵌套（fit2是fit1的子集） 
#卡方值不显著，两模型拟合效果无显著差异


log<-coef(fit2)  #回归系数  对数优势 log（odds）
exp(log)        #指数化   odds

#评价预测变量对结果概率的影响
testdata<-data.frame(rating=c(1,2,3,4,5),age=mean(Affairs$age),
                     yearsmarried=mean(Affairs$yearsmarried),
                     religiousness=mean(Affairs$religiousness))
testdata
testdata$prob<-predict(fit2,newdata = testdata,type = "response")
testdata

#过度离势   Φ=deviance()/df.residual()
deviance(fit2)/df.residual(fit2)  #接近1，表明不存在过度离势
fit.od<-glm(yn~age+yearsmarried+religiousness+rating,
            data=Affairs,family =quasibinomial(link = logit))
#过度离势的检验 H0：Φ=1
pchisq(summary(fit.od)$dispersion*fit2$df.residual,
       fit2$df.residual,lower=F)                     #P>0.05




```


# 随机森林

```{r}
library(randomForest) 
set.seed(1234)
fit.forest <-randomForest(Diagnosis~., data=df.train,na.action=na.roughfix,  importance=TRUE) 
fit.forest 
importance(fit.forest, type=2)

forest.pred <- predict(fit.forest, df.validate) 
t3<-table(df.validate$Diagnosis, forest.pred, dnn=c("Actual", "Predicted")) 
```

# 支持向量机

```{r}
library(e1071)
set.seed(1234) 
fit.svm <- svm(Diagnosis~., data=df.train)
fit.svm
svm.pred <- predict(fit.svm, na.omit(df.validate))
t4<-table(na.omit(df.validate)$Diagnosis,svm.pred, dnn=c("Actual", "Predicted")) 
```

#评估二分类变量准确性

```{r}
performance <- function(table, n=3){  
  if(!all(dim(table) == c(2,2))) stop("Must be a 2 x 2 table")  
  tn = table[1,1] 
  fp = table[1,2]  
  fn = table[2,1]  
  tp = table[2,2]  
  sensitivity = tp/(tp+fn)  
  specificity = tn/(tn+fp) 
  ppp = tp/(tp+fp) 
  npp = tn/(tn+fn) 
  hitrate = (tp+tn)/(tp+tn+fp+fn) 
  result <- paste("Sensitivity = ", round(sensitivity, n) ,  
                  "\nSpecificity = ", round(specificity, n),  
                  "\nPositive Predictive Value = ", round(ppp, n), 
                  "\nNegative Predictive Value = ", round(npp, n), 
                  "\nAccuracy = ", round(hitrate, n), "\n", sep="")  
  cat(result) 
}
performance(t1)
performance(t2)
performance(t3)
performance(t4)

```

```{r}
#| echo: false
source("_comment.R")
```

# Classification

## Libraries

```{r}
library(tidymodels)
library(ISLR2) # For the Default,Smarke, Bikeshare datasets
library(discrim) #discriminant analysis models
library(poissonreg) #Poisson Regression
```

## Datasets

**Default**:default,student,balance,income。前2个是因子，后2个是数值。 **Smarket**：Year,Lag1,Lag2,Lag3,Lag4,Lag5,Volume,Today,Direction。其中*Direction*是因子，其它是数值。

### Default

```{r}
#Figure 4.1
dim(Default)
head(Default) 
ggplot(Default,aes(balance,income))+
  geom_point(aes(shape=default,color=default)) +
  theme(legend.position = "none")|
ggplot(Default,aes(default,balance,fill=default))+
  geom_boxplot()+
  theme(legend.position = "none")+
ggplot(Default,aes(default,income,fill=default))+
  geom_boxplot()+
  theme(legend.position = "none")
```

### Smarket

[corrr](https://corrr.tidymodels.org/) package

```{r}
head(Smarket)
#相关关系矩阵
library(corrr)
cor_Smarket <- Smarket %>%
  select(-Direction) %>%
  correlate()
cor_Smarket

ggplot(Smarket, aes(Year, Volume)) +
  geom_jitter(height = 0)
```

```{r}
#相关关系矩阵可视化
rplot(cor_Smarket, colors = c("indianred2", "black", "skyblue1"))
#?rplot
```

```{r}
#热图形式
library(paletteer)
cor_Smarket %>%
  stretch() %>%
  ggplot(aes(x, y, fill = r)) +
  geom_tile() +
  geom_text(aes(label = as.character(fashion(r)))) +
  scale_fill_paletteer_c("scico::roma", limits = c(-1, 1), direction = -1)
```

## Logistic Regression

```{r}
logit_spec<-logistic_reg(mode = "classification",
                         engine = "glm")
```

### K=2,p=1 binary logistic regression

#### why not linear regression

```{r}
lm_spec<-linear_reg() |> 
  set_mode("regression") |> 
  set_engine("lm")
lm_fit<-lm_spec |> 
  fit(as.numeric(default)-1~balance,data=Default)
lm_fit %>%    
  pluck("fit") %>%   
  summary()

p_lm <- ggplot(Default,aes(balance,as.numeric(default)-1))+
  geom_point(color="orange")+
  geom_smooth(method = "lm",se=FALSE)+
  geom_hline(yintercept = c(0,1))+
  ggtitle("linear regression")
p_lm
```

#### two-class_Y logistic regression

```{r}
logit_fit<-logit_spec |> 
    fit(fct_recode(default, `0`="No",`1`="Yes")~balance,
        data=Default)
logit_fit %>%    
  pluck("fit") %>%   
  summary()
tidy(logit_fit) #prob(Y=1|x)=exp(β0+β1X)/(1+exp(β0+β1X))  -10.6513   0.0055 

p_logit<-Default |> 
  mutate(
    prob=exp(-10.6513+0.0055*balance)/(1+exp(-10.6513+0.0055*balance)),
    logit=log(prob/(1-prob))
  ) |> 
  ggplot()+
    geom_point(aes(balance,as.numeric(default)-1),color="orange")+
    geom_line(aes(balance,prob),color="#1130f9")+  
    geom_hline(yintercept = c(0,1))+
    ggtitle("logistic regression")
p_logit
```

```{r}
p_lm|p_logit #Figure 4.2
```

#### two-class_X logistic regression

```{r}
logit_fit_student<-logit_spec |> 
  fit(fct_recode(default, `0`="No",`1`="Yes")~num_stu,
        data=Default |> 
  mutate(num_stu=as.numeric(student)-1)
  )

tidy(logit_fit_student)
```

### K=2,p\>1 multiple logistic regression

```{r}
logit_spec <- logistic_reg() %>%
  set_mode("classification") %>%
  set_engine("glm") 
```

```{r}

#Default
logit_fit_multiple<-logit_spec |> 
  fit(default~.,data=Default)
tidy(logit_fit_multiple)
augment(logit_fit_multiple, new_data = Default) %>%
  conf_mat(truth = default, estimate = .pred_class) #confusion matrix 混淆矩阵


#Smarket
head(Smarket)
logit_fit <- logit_spec %>%
  fit(
    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
    data = Smarket
    )
#
logit_fit %>%
  pluck("fit") %>%
  summary()
tidy(logit_fit)

#
predict(logit_fit, new_data = Smarket)
predict(logit_fit, new_data = Smarket, type = "prob")

augment(logit_fit, new_data = Smarket) %>%
  conf_mat(truth = Direction, estimate = .pred_class) #confusion matrix 混淆矩阵
#
augment(logit_fit, new_data = Smarket) %>%
  conf_mat(truth = Direction, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

###准确性
augment(logit_fit, new_data = Smarket) %>%
  accuracy(truth = Direction, estimate = .pred_class)
```

fit the model using the **training data** and evaluate it on the **test data**

```{r}
#训练集和测试集
Smarket_train <- Smarket %>%
  dplyr::filter(Year != 2005)

Smarket_test <- Smarket %>%
  dplyr::filter(Year == 2005)
#训练
logit_fit2 <- logit_spec %>%
  fit(
    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
    data = Smarket_train
    )
#评估
augment(logit_fit2, new_data = Smarket_test) %>%
  conf_mat(truth = Direction, estimate = .pred_class) 

augment(logit_fit2, new_data = Smarket_test) %>%
  accuracy(truth = Direction, estimate = .pred_class) 
```

减少弱相关或无关变量

```{r}
logit_fit3 <-logit_spec %>%
  fit(
    Direction ~ Lag1 + Lag2,
    data = Smarket_train
    )

augment(logit_fit3, new_data = Smarket_test) %>%
  conf_mat(truth = Direction, estimate = .pred_class) 

augment(logit_fit3, new_data = Smarket_test) %>%
  accuracy(truth = Direction, estimate = .pred_class) 
```

预测特定值

```{r}
Smarket_new <- tibble(
  Lag1 = c(1.2, 1.5), 
  Lag2 = c(1.1, -0.8)
)
predict(
  logit_fit3,
  new_data = Smarket_new, 
  type = "prob"
)
```

### K\>2,p\>1 multinomial logistic regression

```{r}

```

## Generative Models for Classification

Bayes ' theorem 如 @eq-Bayes 所示。

$$
Pr ( Y=k|X=x)=\dfrac{ {\pi}_k f_k(x)}{\sum_{l=1} ^{k} \pi_{l}f_l(x) }
$$ {#eq-Bayes} 其中，$\pi_k$是某个观测属于第$k$类的先验概率（prior probability）

### Linear Discriminant Analysis

```{r}
library(discrim)
library(ISLR2)
lda_spec <- discrim_linear() %>%
  set_mode("classification") %>%
  set_engine("MASS")
lda_spec

Smarket_train <- Smarket %>%
  dplyr::filter(Year != 2005)
Smarket_test <- Smarket %>%
  dplyr::filter(Year == 2005)

lda_fit <- lda_spec %>%
  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)
lda_fit 
```

```{r}
predict(lda_fit, new_data = Smarket_test)
predict(lda_fit, new_data = Smarket_test, type = "prob")
```

```{r}
augment(lda_fit, new_data = Smarket_test) %>%
  conf_mat(truth = Direction, estimate = .pred_class) 
augment(lda_fit, new_data = Smarket_test) %>%
  accuracy(truth = Direction, estimate = .pred_class) 
```

#### for K=2,p=1

假定$f_k(x)$是正态密度曲线，即 $$
f_k(x)=\dfrac{1}{\sqrt{2\pi}\sigma_k}exp{(-\dfrac{1}{2\sigma_k ^2}(x-\mu_k)^2)}
$${eq-f_k(x)} 并且假定$\sigma_1^2=...=\sigma_k^2$

#### 

### Quadratic Discriminant Analysis

```{r}
qda_spec <- discrim_quad() %>%
  set_mode("classification") %>%
  set_engine("MASS")

qda_fit <- qda_spec %>%
  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)
```

```{r}
augment(qda_fit, new_data = Smarket_test) %>%
  conf_mat(truth = Direction, estimate = .pred_class) 
augment(qda_fit, new_data = Smarket_test) %>%
  accuracy(truth = Direction, estimate = .pred_class) 
```

### Naive Bayes

```{r}

nb_spec <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE)  

nb_fit <- nb_spec %>% 
  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)
```

```{r}
augment(nb_fit, new_data = Smarket_test) %>% 
  conf_mat(truth = Direction, estimate = .pred_class)
augment(nb_fit, new_data = Smarket_test) %>% 
  accuracy(truth = Direction, estimate = .pred_class)
```

```{r}

ggplot(Smarket, aes(Lag1, Lag2)) +
  geom_point(alpha = 0.1, size = 2) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "No apparent correlation between Lag1 and Lag2")
```

## K-Nearest Neighbors

```{r}
knn_spec <- nearest_neighbor(neighbors = 3) %>%
  set_mode("classification") %>%
  set_engine("kknn")

knn_fit <- knn_spec %>%
  fit(Direction ~ Lag1 + Lag2, data = Smarket_train)

knn_fit
```

```{r}
Caravan
```
