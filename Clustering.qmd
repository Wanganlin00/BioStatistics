---
title: "聚类"
---

## Cluster analysis

#聚类分析的一般步骤

1选择合适的变量

2缩放数据 ：标准化

3寻找异常点

4计算距离： dist(x,method = )  默认欧几里得距离

5选择聚类方法和算法

6确定类的数目

7获得最终的聚类解决方案

8结果可视化

9解读类

10验证结果

####                           分层聚类分析(小样本)

算法 

1定义每个观测为一类

2计算每类与其他各类的距离

3把距离最短的两类合并成新的一类,总的类的个数减一

4重复2,3步骤,直到所有的类聚成单个类为止

```{r}
#营养数据的平均联动层次聚类
data("nutrient",package = "flexclust")
nutrient
row.names(nutrient)<-tolower(row.names(nutrient))  #标签转换成小写
nutrient.scaled<-scale(nutrient)  #标准化

d<-dist(nutrient.scaled,method ="euclidean" ) # 默认欧几里得距离
heatmap(as.matrix(d))
hclust.average<-hclust(d,method = "average")
plot(hclust.average,hang=-1,cex=.8,main="Average Linkage Clustering") #树状图


#Nbclust包 
library(NbClust)
nc<-NbClust(nutrient.scaled,distance = "euclidean",
            min.nc = 2,max.nc = 15,method = "average")  #选择聚类的个数
table(nc$Best.nc[1,])
par(no.readonly = TRUE)
par(mfrow=c(1,1))
barplot(table(nc$Best.nc[1,]),
     main="Number of Clusters Chosen by 26 Criteria",
     xlab="Number of Clusters",
     ylab="Number of Criteria")


clusters<-cutree(hclust.average,k=5)  #分成5类
table(clusters)

aggregate(nutrient,by=list(cluster=clusters),median) #描述聚类
aggregate(nutrient.scaled,by=list(cluster=clusters),median)

par(opar)
plot(hclust.average,hang=-1,cex=.8,
     main="Average Linkage Clustering \n5 Cluster Solution")
rect.hclust(hclust.average,k=15)
```

####                             划分K-means聚类

```{r}
data(wine,package = "rattle")
head(wine,5)
df<-scale(wine[-1])
wssplot<-function(data,nc=15,seed=1234){ #Kmeans平方和对聚类数量曲线
  wss<-(nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc) {
    set.seed(seed)
    wss[i]<-sum(kmeans(data,centers = i)$withinss)}
  plot(1:nc,wss,type="b",xlab="Number of Ckusters",
       ylab="Within groups sum of squares")
}

wssplot(df)  #类中的Kmeans平方和对聚类数量曲线 类似碎石图14章
library(NbClust)
set.seed(1234)
nc<-NbClust(df,min.nc = 2,max.nc = 15,method = "kmeans")  #决定聚类个数
table(nc$Best.nc[1,])
barplot(table(nc$Best.nc[1,]),
        main="Number of Clusters Chosen by 26 Criteria",
        xlab="Number of Clusters",
        ylab="Number of Criteria")

set.seed(1234)
fit.km<-kmeans(df,centers = 3,nstart = 25)     #K均值聚类分析
fit.km$size
fit.km$centers
aggregate(wine[-1],by=list(clust=fit.km$cluster),mean)

ct.km<-table(wine$Type,fit.km$cluster) #与实际分类的拟合差异
ct.km
library(flexclust)
randIndex(ct.km)  #Rand Index 量化类型变量和类之间的关系


#对葡萄酒数据使用围绕中心点的划分方法
library(cluster)
set.seed(1234)
fit.pam<-pam(wine[-1],k=3,stand = TRUE)  #围绕中心点
fit.pam$id.med                      #输出中心点
clusplot(fit.pam,main="Bivaritate Cluster Plot")
ct.pam<-table(wine$Type,fit.pam$clustering)
ct.pam
randIndex(ct.pam) 


#### 避免不存在的类    ####

library(fMultivar)
set.seed(1234)
df<-rnorm2d(1000,rho = .5)
df<-as.data.frame(df)
plot(df)
wssplot(df)
library(NbClust)
nc2<-NbClust(df,min.nc = 2,max.nc = 15,method = "kmeans")
dev.new()
barplot(table(nc2$Best.nc[1,]))

library(ggplot2)
library(cluster)
fit<-pam(df,k=2)
df$clustering<-factor(fit$clustering)
ggplot(data=df,aes(x=V1,y=V2,color=clustering,shape=clustering))+
  geom_point()+ggtitle("Clustering of Bivaritate Normal Data")
dev.new()
plot(nc2$All.index[,4],type="o",ylab = "CCC", #第4列是CCC值
     xlab="Number of Clusters",col="blue")

```



```{r}
source("_comment.R")
```

```{r}
#| message: false
library(tidymodels)
library(tidyclust)
library(patchwork)
library(ISLR2)
```

# tidy Clustering
## simulated dataset

```{r}
set.seed(2)
x_df <- tibble(
  V1 = rnorm(n = 50, mean = rep(c(0, 3), each = 25)),
  V2 = rnorm(n = 50, mean = rep(c(0, -4), each = 25))
)
```

```{r}
#| fig-alt: |
#|  x_df数据集的散点图，其中 V1 位于 x 轴，V2 位于 y 
#|  轴上。与数据中的两组相对应的颜色。数据整齐地分成高斯聚类。
#|  
x_df %>%
  ggplot(aes(V1, V2, color = rep(c("A", "B"), each = 25))) +
  geom_point() +
  labs(color = "groups")
```

## Kmeans Clustering


### kmeans specification

通过使用`num_clusters`参数指定K-means算法需要使用多少个质心（centroids）。我们还设置了模式和引擎，这次设置为与默认值相同。我们还设置了 `nstart = 20`，这允许算法具有多个初始起始位置，我们使用这些位置来希望找到全局最大值而不是局部最大值

```{r}
kmeans_spec <- k_means(num_clusters = 3) %>%
  set_mode("partition") %>%
  set_engine("stats") %>%
  set_args(nstart = 20)
kmeans_spec
```

### clustering

K-means algorithm starts with random initialization

```{r}
set.seed(1234)
kmeans_fit <- kmeans_spec %>%
  fit(~., data = x_df)
```

### extracting information

```{r}
kmeans_fit
```

```{r}
extract_fit_summary(kmeans_fit)
```

```{r}
extract_centroids(kmeans_fit)
```

```{r}
extract_cluster_assignment(kmeans_fit)
```

### predicating

聚类分析模型中的预测定义不明确。但我们可以将其视为"如果这些观察结果是数据集的一部分，它们会属于哪个集群"。对于 k 均值情况，它着眼于这些观测值最接近的质心。

```{r}
predict(kmeans_fit, new_data = x_df)
augment(kmeans_fit, new_data = x_df)
```

```{r}
#| fig-alt: |
#|  augmented 数据集的散点图，其中 V1 位于 x轴上，V2
#| 在 y 轴上。颜色与.pred_cluster变量相对应。
#| 最左边的簇是一种颜色，最右边的簇是另一种颜色
#| 它们之间的点是第三种颜色。
augment(kmeans_fit, new_data = x_df) %>%
  ggplot(aes(V1, V2, color = .pred_cluster)) +
  geom_point()
```

### `tune_cluster()`找到最适合的类的数目

```{r}
kmeans_spec_tuned <- kmeans_spec %>% 
  set_args(num_clusters = tune())

kmeans_wf <- workflow() %>%
  add_model(kmeans_spec_tuned) %>%
  add_formula(~.)
```

```{r}
set.seed(1234)
x_boots <- bootstraps(x_df, times = 10)

num_clusters_grid <- tibble(num_clusters = seq(1, 10))

tune_res <- tune_cluster(
  object = kmeans_wf,
  resamples = x_boots,
  grid = num_clusters_grid
)
```

```{r}
tune_res %>%
  collect_metrics()
```

plot against `k` so we can use the [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) to find the optimal number of clusters.

```{r}
tune_res %>%
  autoplot()
```

### final Kmeans

```{r}
final_kmeans <- kmeans_wf %>%
  update_model(kmeans_spec %>% set_args(num_clusters = 2)) %>%
  fit(x_df)
```

### visualizing

```{r}
#| fig-alt: |
#| 增强数据集的散点图，其中 V1 位于 x 轴上，V2在 y 
#| 轴上。颜色对应于数据。这些结果与真实聚类非常吻合。
augment(final_kmeans, new_data = x_df) %>%
  ggplot(aes(V1, V2, color = .pred_cluster)) +
  geom_point()
```

## Hierarchical Clustering

### hclust specification

```{r}
res_hclust_complete <- hier_clust(linkage_method = "complete") %>%
  fit(~., data = x_df)

res_hclust_average <- hier_clust(linkage_method = "average") %>%
  fit(~., data = x_df)

res_hclust_single <- hier_clust(linkage_method = "single") %>%
  fit(~., data = x_df)
```

### extracting model and visualizing

[factoextra](https://rpkgs.datanovia.com/factoextra/) package

```{r}
#| warning: false
#| fig-alt: |
#| 树状图可视化
library(factoextra)
res_hclust_complete %>%
  extract_fit_engine() %>%
  fviz_dend(main = "complete", k = 2)
```

```{r}
#| warning: false
#| fig-alt: |
#| 树状图可视化
res_hclust_average %>%
  extract_fit_engine() %>%
  fviz_dend(main = "average", k = 2)
```

```{r}
#| warning: false
#| fig-alt: |
#| 树状图可视化
res_hclust_single %>%
  extract_fit_engine() %>%
  fviz_dend(main = "single", k = 2)
```
### recipe and workflow
```{r}
#| warning: false
#| fig-alt: |
#|  Dendrogram visualization. Both left and right side looks more
#|  or less even.
hier_rec <- recipe(~., data = x_df) %>%
  step_normalize(all_numeric_predictors()) #缩放

hier_wf <- workflow() %>%
  add_recipe(hier_rec) %>%
  add_model(hier_clust(linkage_method = "complete"))

hier_fit <- hier_wf %>%
  fit(data = x_df) 

hier_fit %>%
  extract_fit_engine() %>%
  fviz_dend(k = 2)
```

