# 降维

## 矩阵分解

```{r}
set.seed(55)
e <- c(
  x=c(runif(20,1,9)),
  y=c(runif(30,40,50)),
  z=c(runif(50,90,100))
)
e
mat<-matrix(e,nrow = 10,byrow = TRUE)
mat

mat_center <- mat-rowMeans(mat)
# 奇异值分解
svt1 <- svd(mat_center)
svt1

plot(svt1$d)
plot(svt1$d^2/sum(svt1$d^2))
par(mfrow=c(1,2))

svt1$v
plot(svt1$v[,1],ylab = "1st PC")
plot(svt1$v[,2],ylab="2nd PC")

plot(svt1$v[,1],svt1$v[,2],xlab="lst PC",ylab="2nd PC")

```

```{r}
library(ggfortify)
autoplot(prcomp(mat),
         data=mat,
         )
```

## 线性降维

### Principal component analysis(PCA)

```{r}
# PC1=a1X1+a2X2+...+akXk

library(psych)

#判断主成分的个数 

#Cattell碎石图        图形变化最大处。即拐角处
#Kaiser-Harris准则    特征值大于1，直线y=1以上
#平行分析             基于真实数据的特征值大于一组随机数据矩阵相应的特征值（虚线）

fa.parallel(USJudgeRatings[,-1],fa="fa",n.iter = 100,show.legend = TRUE,
            main = "Scree Plot with parallel ananlysis")  
fa.parallel(Harman23.cor$cov,n.obs = 302,fa="pc",n.iter = 100,
            show.legend = TRUE,main=  "Scree Plot with parallel ananlysis")

#提取主成分
pc.1<-principal(USJudgeRatings[,-1],nfactors = 1,rotate = "varimax",scores = FALSE)
pc.1

pc.2<-principal(Harman23.cor$cov,nfactors = 2,rotate="none")
pc.2

#主成分旋转
rpc.2<-principal(Harman23.cor$cov,nfactors = 2,rotate="varimax")
rpc.2

#获取主成分得分或系数
pc.1<-principal(USJudgeRatings[,-1],nfactors = 1,scores = TRUE)
pc.1$scores                         #主成分得分
cor(USJudgeRatings$CONT,pc.1$scores)#相关系数

round(rpc.2$weights,2)  #主成分得分系数(标准化的回归权重)

```

### 因子分析

```{r}
# Xi=a1F1+a2F2+...+apFp+Ui        k个可观测变量，p个公共因子

options(digits = 2)
cor<-cov2cor(ability.cov$cov)  #相关关系矩阵
cor


#Cattell碎石图        图形变化最大处。即拐角处
#Kaiser-Harris准则    特征值大于0，直线y=0以上
#平行分析             基于真实数据的特征值大于一组随机数据矩阵相应的特征值（虚线）

fa.parallel(cor,n.obs = 112,fa="both",n.iter = 100)#判断公共因子的个数
fa(cor,nfactors = 2,rotate = "none",fm="pa")  #提取公共因子

#旋转
fa.varimax<-fa(cor,nfactors = 2,rotate = "varimax",fm="pa") #正交旋转
fa.varimax
install.packages("GPArotation")
library(GPArotation)
fa.promax<-fa(cor,nfactors = 2,rotate = "promax",fm="pa") #斜交旋转
fa.promax
unclass(fa.promax$loadings) %*% fa.promax$Phi    #  矩阵乘法 A %*% B
#因子结构矩阵=因子模式矩阵*因子关联矩阵


fa.plot(fa.promax,labels = rownames(fa.promax$loadings)) #斜交旋转结果图
fa.diagram(fa.promax)

#因子得分
fa.promax<-fa(cor,nfactors = 2,rotate = "promax",fm="pa",scores = TRUE) #斜交旋转
fa.promax$score
fa.promax$weights

```

## tidy PCA

```{r include=FALSE}
library(tidymodels)
```

`USArrests` data set

```{r}
USArrests <- as_tibble(USArrests, rownames = "state")
USArrests
```

the mean of each of the variables

```{r}
USArrests %>%
  select(-state) %>%
  map_dfr(mean)  #apply(.,2,mean)
```

perform PCA

```{r}
USArrests_pca <- USArrests %>%
  select(-state) %>%
  prcomp(scale = TRUE)

USArrests_pca

broom::tidy(USArrests_pca)
tidy(USArrests_pca, matrix = "scores") #by default  USArrests_pca$x 变长表
tidy(USArrests_pca, matrix = "loadings") # USArrests_pca$Rotation 变长表
```

```{r}
#| fig-alt: |
#|   Facetted barchart of the principal component loadings.
#|   The 4 variables are shown across the y-axis and the amount
#|   of the loading is show as the bar height across the x-axis.
#|   The 4 variables: UnbanPop, Rape, Murder and Assault are more
#|   or less evenly represented in the first loading, with 
#|   UnbanPop least. Second loading has UnbanPop highest, third
#|   loading has Rape highest. Murder and Assult highest in forth
#|   and final loading.
tidy(USArrests_pca, matrix = "loadings") %>%
  ggplot(aes(value, column)) +
  facet_wrap(~ PC) +
  geom_col() +
  scale_x_continuous(labels = scales::percent)
```

```{r}
#standard deviation, percent explanation and cumulative explanation
tidy(USArrests_pca, matrix = "eigenvalues") 
```

```{r}
#| fig-alt: |
#|   Bar chart of percent standard deviation explained for the
#|   4 principal components. First PC is a little over 60%, second
#|   is at around 25%, third is a little under 10% and forth is at
#|   around 5%.
tidy(USArrests_pca, matrix = "eigenvalues") %>%
  ggplot(aes(PC, percent)) +
  geom_col()
```

```{r}
augment(USArrests_pca)

augment(USArrests_pca, newdata = USArrests[1:5, ])
```

```{r}
pca_rec <- recipe(~., data = USArrests) %>%
  step_normalize(all_numeric()) %>%
  step_pca(all_numeric(), id = "pca") %>%
  prep()

pca_rec %>%
  bake(new_data = NULL)

pca_rec %>%
  bake(new_data = USArrests[40:45, ])
```

```{r}
tidy(pca_rec, id = "pca", type = "coef")
tidy(pca_rec, id = "pca", type = "variance")
```

```{r}
recipe(~., data = USArrests) %>%
  step_normalize(all_numeric()) %>%
  step_pca(all_numeric(), num_comp = 3) %>%
  prep() %>%
  bake(new_data = NULL)
```

```{r}
recipe(~., data = USArrests) %>%
  step_normalize(all_numeric()) %>%
  step_pca(all_numeric(), threshold = 0.7) %>%
  prep() %>%
  bake(new_data = NULL)
```

## 非线性降维

### t-Distributed Stochastic Neighbor Embedding (t-SNE)

t-Distributed Stochastic Neighbor Embedding (t-SNE)是一种降维技术，**用于在二维或三维的低维空间中表示高维数据集，从而使其可视化**。与其他降维算法(如PCA)相比，t-SNE创建了一个缩小的特征空间，相似的样本由附近的点建模，不相似的样本由高概率的远点建模。

### Uniform Manifold Approximation and Projection(umap)

2018年McInnes提出了算法，UMAP（Uniform Manifold Approximation and Projection for Dimension Reduction，一致的流形逼近和投影以进行降维）。 一致的流形近似和投影（UMAP）是一种降维技术，类似于t-SNE，可用于可视化，但也可用于一般的非线性降维。 该算法基于关于数据的三个假设：

-   数据均匀分布在黎曼流形上（Riemannian manifold）；

-   黎曼度量是局部恒定的（或可以这样近似）；

-   流形是局部连接的。
