# 降维

## 矩阵分解
```{r}
set.seed(55)
e <- c(
  x=c(runif(20,1,9)),
  y=c(runif(30,40,50)),
  z=c(runif(50,90,100))
)
e
mat<-matrix(e,nrow = 10,byrow = TRUE)
mat

mat_center <- mat-rowMeans(mat)
### 奇异值分解
svt1 <- svd(mat_center)
svt1

plot(svt1$d)
plot(svt1$d^2/sum(svt1$d^2))
par(mfrow=c(1,2))

svt1$v
plot(svt1$v[,1],ylab = "1st PC")
plot(svt1$v[,2],ylab="2nd PC")

plot(svt1$v[,1],svt1$v[,2],xlab="lst PC",ylab="2nd PC")

```
```{r}
library(ggfortify)
autoplot(prcomp(mat),
         data=mat,
         )
```


## 线性降维

### Principal component analysis(PCA)

```{r}
# PC1=a1X1+a2X2+...+akXk

library(psych)

#判断主成分的个数 

#Cattell碎石图        图形变化最大处。即拐角处
#Kaiser-Harris准则    特征值大于1，直线y=1以上
#平行分析             基于真实数据的特征值大于一组随机数据矩阵相应的特征值（虚线）

fa.parallel(USJudgeRatings[,-1],fa="fa",n.iter = 100,show.legend = TRUE,
            main = "Scree Plot with parallel ananlysis")  
fa.parallel(Harman23.cor$cov,n.obs = 302,fa="pc",n.iter = 100,
            show.legend = TRUE,main=  "Scree Plot with parallel ananlysis")

#提取主成分
pc.1<-principal(USJudgeRatings[,-1],nfactors = 1,rotate = "varimax",scores = FALSE)
pc.1

pc.2<-principal(Harman23.cor$cov,nfactors = 2,rotate="none")
pc.2

#主成分旋转
rpc.2<-principal(Harman23.cor$cov,nfactors = 2,rotate="varimax")
rpc.2

#获取主成分得分或系数
pc.1<-principal(USJudgeRatings[,-1],nfactors = 1,scores = TRUE)
pc.1$scores                         #主成分得分
cor(USJudgeRatings$CONT,pc.1$scores)#相关系数

round(rpc.2$weights,2)  #主成分得分系数(标准化的回归权重)

```



### 因子分析

```{r}
# Xi=a1F1+a2F2+...+apFp+Ui        k个可观测变量，p个公共因子

options(digits = 2)
cor<-cov2cor(ability.cov$cov)  #相关关系矩阵
cor


#Cattell碎石图        图形变化最大处。即拐角处
#Kaiser-Harris准则    特征值大于0，直线y=0以上
#平行分析             基于真实数据的特征值大于一组随机数据矩阵相应的特征值（虚线）

fa.parallel(cor,n.obs = 112,fa="both",n.iter = 100)#判断公共因子的个数
fa(cor,nfactors = 2,rotate = "none",fm="pa")  #提取公共因子

#旋转
fa.varimax<-fa(cor,nfactors = 2,rotate = "varimax",fm="pa") #正交旋转
fa.varimax
install.packages("GPArotation")
library(GPArotation)
fa.promax<-fa(cor,nfactors = 2,rotate = "promax",fm="pa") #斜交旋转
fa.promax
unclass(fa.promax$loadings) %*% fa.promax$Phi    #  矩阵乘法 A %*% B
#因子结构矩阵=因子模式矩阵*因子关联矩阵


fa.plot(fa.promax,labels = rownames(fa.promax$loadings)) #斜交旋转结果图
fa.diagram(fa.promax)

#因子得分
fa.promax<-fa(cor,nfactors = 2,rotate = "promax",fm="pa",scores = TRUE) #斜交旋转
fa.promax$score
fa.promax$weights

```


## tidy PCA


```{r include=FALSE}
library(tidymodels)
```

`USArrests` data set

```{r}
USArrests <- as_tibble(USArrests, rownames = "state")
USArrests
```
 
 the mean of each of the variables
 
```{r}
USArrests %>%
  select(-state) %>%
  map_dfr(mean)  #apply(.,2,mean)
```

 perform PCA
 
```{r}
USArrests_pca <- USArrests %>%
  select(-state) %>%
  prcomp(scale = TRUE)

USArrests_pca

broom::tidy(USArrests_pca)
tidy(USArrests_pca, matrix = "scores") #by default  USArrests_pca$x 变长表
tidy(USArrests_pca, matrix = "loadings") # USArrests_pca$Rotation 变长表
```

```{r}
#| fig-alt: |
#|   Facetted barchart of the principal component loadings.
#|   The 4 variables are shown across the y-axis and the amount
#|   of the loading is show as the bar height across the x-axis.
#|   The 4 variables: UnbanPop, Rape, Murder and Assault are more
#|   or less evenly represented in the first loading, with 
#|   UnbanPop least. Second loading has UnbanPop highest, third
#|   loading has Rape highest. Murder and Assult highest in forth
#|   and final loading.
tidy(USArrests_pca, matrix = "loadings") %>%
  ggplot(aes(value, column)) +
  facet_wrap(~ PC) +
  geom_col() +
  scale_x_continuous(labels = scales::percent)
```

```{r}
#standard deviation, percent explanation and cumulative explanation
tidy(USArrests_pca, matrix = "eigenvalues") 
```
```{r}
#| fig-alt: |
#|   Bar chart of percent standard deviation explained for the
#|   4 principal components. First PC is a little over 60%, second
#|   is at around 25%, third is a little under 10% and forth is at
#|   around 5%.
tidy(USArrests_pca, matrix = "eigenvalues") %>%
  ggplot(aes(PC, percent)) +
  geom_col()
```
```{r}
augment(USArrests_pca)

augment(USArrests_pca, newdata = USArrests[1:5, ])
```

```{r}
pca_rec <- recipe(~., data = USArrests) %>%
  step_normalize(all_numeric()) %>%
  step_pca(all_numeric(), id = "pca") %>%
  prep()

pca_rec %>%
  bake(new_data = NULL)

pca_rec %>%
  bake(new_data = USArrests[40:45, ])
```
```{r}
tidy(pca_rec, id = "pca", type = "coef")
tidy(pca_rec, id = "pca", type = "variance")
```

```{r}
recipe(~., data = USArrests) %>%
  step_normalize(all_numeric()) %>%
  step_pca(all_numeric(), num_comp = 3) %>%
  prep() %>%
  bake(new_data = NULL)
```

```{r}
recipe(~., data = USArrests) %>%
  step_normalize(all_numeric()) %>%
  step_pca(all_numeric(), threshold = 0.7) %>%
  prep() %>%
  bake(new_data = NULL)
```















## 非线性降维

### Uniform Manifold Approximation and Projection(umap)

2018年McInnes提出了算法，UMAP（Uniform Manifold Approximation and Projection for Dimension Reduction，一致的流形逼近和投影以进行降维）。 一致的流形近似和投影（UMAP）是一种降维技术，类似于t-SNE，可用于可视化，但也可用于一般的非线性降维。 该算法基于关于数据的三个假设：

-   数据均匀分布在黎曼流形上（Riemannian manifold）；

-   黎曼度量是局部恒定的（或可以这样近似）；

-   流形是局部连接的。

根据这些假设，可以对具有模糊拓扑结构的流形进行建模。 通过搜索具有最接近的等效模糊拓扑结构的数据的低维投影来找到嵌入。

相对于t-SNE，其主要特点：降维快准狠。

论文：McInnes, L, Healy, J, UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, ArXiv e-prints 1802.03426, 2018

同时其作者开源实现代码。

### t-Distributed Stochastic Neighbor Embedding (t-SNE)

t-Distributed Stochastic Neighbor Embedding (t-SNE)是一种降维技术，**用于在二维或三维的低维空间中表示高维数据集，从而使其可视化**。与其他降维算法(如PCA)相比，t-SNE创建了一个缩小的特征空间，相似的样本由附近的点建模，不相似的样本由高概率的远点建模。

在高水平上，t-SNE为高维样本构建了一个概率分布，相似的样本被选中的可能性很高，而不同的点被选中的可能性极小。然后，t-SNE为低维嵌入中的点定义了相似的分布。最后，t-SNE最小化了两个分布之间关于嵌入点位置的Kullback-Leibler（KL)散度。

> t-SNE是一种集降维与可视化于一体的技术，它是基于SNE可视化的改进，解决了SNE在可视化后样本分布拥挤、边界不明显的特点，是目前较好的降维可视化手段。

t-SNE是目前来说效果最好的数据降维与可视化方法，但是它的缺点也很明显，比如：

-   占内存大，运行时间长。

-   专用于可视化，即嵌入空间只能是2维或3维。

-   需要尝试不同的初始化点，以防止局部次优解的影响。

<https://zhuanlan.zhihu.com/p/64664346>

##### **t-SNE优点**

流形学习中其他方法如Isomap、LLE等，主要用于展开单个连续的低维流形（比如"瑞士卷"数据集），而t-SNE主要用于数据的局部结构，并且会倾向于提取出局部的簇，这种能力对于可视化同时包含多个流形的高维数据（比如MNIST数据集）很有效。

##### **t-SNE缺点**

➊时间、空间复杂度为O(n\^2)，计算代价昂贵。百万量级的数据需要几小时，对于PCA可能只需要几分钟。

➋升级版Barnes-Hut t-SNE可以让复杂度降为O(nlogn)，但只限于获得二维和三维的嵌入。（sklearn中可以直接使用参数method='barnes_hut'）

➌由于代价函数非凸，多次执行算法的结果是随机的（名字中"Stochatsic"的由来？），需要多次运行选取最好的结果。

➍全局结构不能很清楚的保留。这个问题可以通过先用PCA降维到一个合理的维度（如50）后再用t-SNE来缓解，前置的PCA步骤也可以起到去除噪声等功能。（sklearn中可以直接使用参数init='pca'）
